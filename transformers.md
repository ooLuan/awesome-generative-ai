# ğŸ¤– Transformers & Foundation Models

A curated collection of widely-used transformer models across NLP, multimodal, and large language model (LLM) domains.

---

## ğŸ“š Natural Language Transformers

### ğŸ”¹ [BERT (Bidirectional Encoder Representations from Transformers)](https://github.com/google-research/bert)
- Pretrained on masked language modeling and next sentence prediction
- Foundation for many downstream NLP tasks

### ğŸ”¹ [RoBERTa](https://github.com/pytorch/fairseq/tree/main/examples/roberta)
- Robustly optimized BERT variant by Facebook AI
- Trained longer, with more data and no NSP objective

### ğŸ”¹ [ELECTRA](https://github.com/google-research/electra)
- More efficient pretraining via replaced token detection
- Outperforms BERT on many tasks with fewer compute

### ğŸ”¹ [KoBERT](https://github.com/SKTBrain/KoBERT)
- Korean BERT pretrained with Subword embeddings
- Optimized for downstream Korean NLP tasks

### ğŸ”¹ [DistilBERT](https://github.com/huggingface/transformers)
- A distilled version of BERT that is 60% faster and smaller

---

## ğŸ§  Large Language Models (LLMs)

### ğŸ”¹ [GPT-2 & GPT-3 (OpenAI)](https://platform.openai.com/docs)
- Autoregressive transformer models for text generation
- API-based usage for chat, summarization, QA, etc.

### ğŸ”¹ [LLaMA (Meta)](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- Open-weight LLMs with competitive performance
- LLaMA 2 and 3 available for research and commercial use

### ğŸ”¹ [Mistral](https://mistral.ai/news/announcing-mistral-7b/)
- Lightweight open-weight models (Mistral 7B)
- State-of-the-art on many benchmarks with low latency

### ğŸ”¹ [Falcon (TII)](https://huggingface.co/tiiuae/falcon-40b)
- 7B / 40B parameter LLMs for multilingual generation
- Ranked high in Open LLM Leaderboards

### ğŸ”¹ [Claude (Anthropic)](https://www.anthropic.com/index/introducing-claude)
- Alignment-first LLMs with long context window
- Claude 3 supports structured tool use and reasoning

### ğŸ”¹ [Gemini (Google DeepMind)](https://ai.google.dev/gemini-api/docs)
- Multimodal LLM supporting vision, audio, and code inputs
- Combines capabilities of PaLM + DeepMind Alpha

---

## ğŸ–¼ï¸ Vision & Multimodal Transformers

### ğŸ”¹ [ViT (Vision Transformer)](https://github.com/google-research/vision_transformer)
- Pure transformer architecture for image classification

### ğŸ”¹ [BLIP-2](https://github.com/salesforce/BLIP)
- Vision-language pretraining for image captioning and VQA

### ğŸ”¹ [FLAN-T5](https://huggingface.co/google/flan-t5-xxl)
- Instruction-tuned T5 model by Google
- Works well for prompt-based inference tasks

---

ğŸ’¡ **Tip**: Use HuggingFace Transformers library for seamless loading and fine-tuning of these models.


