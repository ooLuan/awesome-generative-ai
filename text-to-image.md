# ğŸ–¼ï¸ Text-to-Image Generation

This section provides a curated list of recent and powerful models that convert natural language into images.

---

## ğŸ¨ Notable Models

### ğŸ”· [ControlNet](https://github.com/lllyasviel/ControlNet)
- Add structural control (pose, depth, edge) to pre-trained diffusion models
- Works with Stable Diffusion and other backbones

### ğŸ”· [T2I-Adapter](https://github.com/TencentARC/T2I-Adapter)
- Plug-in modules to guide image synthesis with external conditions

### ğŸ”· [DreamBooth](https://github.com/XavierXiao/Dreambooth-Stable-Diffusion)
- Fine-tune a diffusion model to generate specific subjects
- Few-shot personalized image generation

### ğŸ”· [StyleDiffusion](https://github.com/MatthewLWang/StyleDiffusion)
- Text-guided style transfer in diffusion synthesis

### ğŸ”· [Sana](https://github.com/NVlabs/Sana)
- Scalable personalization in diffusion with multi-subject support

### ğŸ”· [Infinity](https://github.com/FoundationVision/Infinity)
- Infinite resolution image generation using patch-wise modeling

---

## ğŸ§ª Emerging Ideas

- [IMAG-Dressing](https://github.com/muzishen/IMAGDressing) â€“ Text-driven virtual try-on
- [Text-to-Pose-to-Image](https://github.com/clement-bonnet/text-to-pose) â€“ Improves diffusion with pose guidance
- [Rich-Text-to-Image](https://github.com/songweige/rich-text-to-image) â€“ Enhanced prompt conditioning
- [custom-diffusion](https://github.com/adobe-research/custom-diffusion) â€“ Multi-concept personalization

---

ğŸ’¡ **Tip**: Combine T2I-Adapter or ControlNet with Stable Diffusion for controllable and reliable synthesis.

